{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeadDkMiISin"
      },
      "source": [
        "# Gemini API: Function calling with Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df1767a3d1cc"
      },
      "source": [
        " Function calling lets developers create a description of a function in their code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with. Function calling lets you use functions as tools in generative AI applications, and you can define more than one function within a single request.\n",
        "\n",
        "This notebook provides code examples to help you get started. The documentation's [quickstart](https://ai.google.dev/gemini-api/docs/function-calling#python) is also a good place to start understanding function calling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY2NtS3jV56U"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5027929de8f"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-hHZfLZ7FfH"
      },
      "source": [
        "### Set up your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](../quickstarts/Authentication.ipynb) quickstart for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ab9ASynfcIZn"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpaKynP8qLw1"
      },
      "source": [
        "### Choose a model\n",
        "\n",
        "Function calling should work with all the [Gemini 2.0](https://ai.google.dev/gemini-api/docs/models/gemini-v2) models with the GenAI SDK. It also works with the 1.5 generation of models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "sEK4ZDVGqJ5H"
      },
      "outputs": [],
      "source": [
        "MODEL_ID=\"gemini-2.5-flash\" # @param [\"gemini-2.5-flash\", \"gemini-2.5-pro\", \"gemini-2.0-flash\", \"gemini-2.5-flash-lite-preview-06-17\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f383614ec30"
      },
      "source": [
        "## Setting up Functions as Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b82c1aecb657"
      },
      "source": [
        "To use function calling, pass a list of functions to the `tools` parameter when creating a [`GenerativeModel`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel). The model uses the function name, docstring, parameters, and parameter type annotations to decide if it needs the function to best answer a prompt.\n",
        "\n",
        "> Important: The SDK converts function parameter type annotations to a format the API understands (`genai.types.FunctionDeclaration`). The API only supports a limited selection of parameter types, and the Python SDK's automatic conversion only supports a subset of that: `AllowedTypes = int | float | bool | str | list['AllowedTypes'] | dict`\n",
        "\n",
        "\n",
        "**Example: Lighting System Functions**\n",
        "\n",
        "Here are 3 functions controlling a hypothetical lighting system. Note the docstrings and type hints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "C8J_H1hSp4m-"
      },
      "outputs": [],
      "source": [
        "def enable_lights():\n",
        "    \"\"\"เปิดระบบไฟส่องสว่าง (Turn on the lighting system)\"\"\"\n",
        "    print(\"LIGHTBOT: Lights enabled.\")\n",
        "\n",
        "\n",
        "def set_light_color(rgb_hex: str):\n",
        "    \"\"\"ตั้งค่าสีของไฟ (Set the light color). ต้องเปิดไฟก่อนถึงจะเปลี่ยนสีได้ (Lights must be enabled for this to work)\"\"\"\n",
        "    print(f\"LIGHTBOT: Lights set to {rgb_hex}.\")\n",
        "\n",
        "def stop_lights():\n",
        "    \"\"\"ปิดไฟ (Stop flashing lights)\"\"\"\n",
        "    print(\"LIGHTBOT: Lights turned off.\")\n",
        "\n",
        "light_controls = [enable_lights, set_light_color, stop_lights]\n",
        "instruction = \"\"\"\n",
        "  คุณคือบอทควบคุมระบบไฟที่เป็นมิตร (You are a helpful lighting system bot). คุณสามารถเปิด-ปิดไฟ และตั้งค่าสีไฟได้ (You can turn lights on and off, and you can set the color). ห้ามทำงานอื่นนอกเหนือจากนี้ (Do not perform any other tasks).\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry0JsK405KwS"
      },
      "source": [
        "## Basic Function Calling with Chat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l4wdq8b5Nuy"
      },
      "source": [
        "Function calls naturally fit into multi-turn conversations. The Python SDK's `ChatSession (client.chats.create(...))` is ideal for this, as it automatically handles conversation history.\n",
        "\n",
        "Furthermore, `ChatSession` simplifies function calling execution via its `automatic_function_calling` feature (enabled by default), which will be explored more later. For now, let's see a basic interaction where the model decides to call a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-yuQ2gCY5ujD"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(\n",
        "    model=MODEL_ID,\n",
        "    config={\n",
        "        \"tools\": light_controls,\n",
        "        \"system_instruction\": instruction,\n",
        "    }\n",
        ")\n",
        "\n",
        "response = chat.send_message(\"It's awful dark in here...\")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1UsMG3FqYrC"
      },
      "source": [
        "## Examining Function Calls and Execution History\n",
        "\n",
        "To understand what happened in the background, you can examine the chat history.\n",
        "\n",
        "The `Chat.history` property stores a chronological record of the conversation between the user and the Gemini model. You can get the history using `Chat.get_history()`. Each turn in the conversation is represented by a `genai.types.Content` object, which contains the following information:\n",
        "\n",
        "**Role**: Identifies whether the content originated from the \"user\" or the \"model\".\n",
        "\n",
        "**Parts**: A list of genai.types.Part objects that represent individual components of the message. With a text-only model, these parts can be:\n",
        "\n",
        "* **Text**: Plain text messages.\n",
        "* **Function Call (genai.types.FunctionCall)**: A request from the model to execute a specific function with provided arguments.\n",
        "* **Function Response (genai.types.FunctionResponse)**: The result returned by the user after executing the requested function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "SBNAqSexqZlZ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "def print_history(chat):\n",
        "  for content in chat.get_history():\n",
        "      display(Markdown(\"###\" + content.role + \":\"))\n",
        "      for part in content.parts:\n",
        "          if part.text:\n",
        "              display(Markdown(part.text))\n",
        "          if part.function_call:\n",
        "              print(\"Function call: {\", part.function_call, \"}\")\n",
        "          if part.function_response:\n",
        "              print(\"Function response: {\", part.function_response, \"}\")\n",
        "      print(\"-\" * 80)\n",
        "\n",
        "print_history(chat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS84-2yG7A--"
      },
      "source": [
        "This history shows the flow:\n",
        "\n",
        "1. **User**: Sends the message.\n",
        "\n",
        "2. **Model**: Responds not with text, but with a `FunctionCall` requesting `enable_lights`.\n",
        "\n",
        "3. **User (SDK)**: The `ChatSession` automatically executes `enable_lights()` because `automatic_function_calling` is enabled. It sends the result back as a `FunctionResponse`.\n",
        "\n",
        "4. **Model**: Uses the function's result (\"Lights enabled.\") to formulate the final text response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsCZArT47p5T"
      },
      "source": [
        "## Automatic Function Execution (Python SDK Feature)\n",
        "\n",
        "As demonstrated above, the `ChatSession` in the Python SDK has a powerful feature called Automatic Function Execution. When enabled (which it is by default), if the model responds with a FunctionCall, the SDK will:\n",
        "\n",
        "1. Find the corresponding Python function in the provided `tools`.\n",
        "\n",
        "2. Execute the function with the arguments provided by the model.\n",
        "\n",
        "3. Send the function's return value back to the model in a `FunctionResponse`.\n",
        "\n",
        "4. Return only the model's final response (usually text) to your code.\n",
        "\n",
        "This significantly simplifies the workflow for common use cases.\n",
        "\n",
        "**Example: Math Operations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "r1FnK3EB8jgQ"
      },
      "outputs": [],
      "source": [
        "from google.genai import types # Ensure types is imported\n",
        "\n",
        "def add(a: float, b: float):\n",
        "    \"\"\"returns a + b.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "def subtract(a: float, b: float):\n",
        "    \"\"\"returns a - b.\"\"\"\n",
        "    return a - b\n",
        "\n",
        "def multiply(a: float, b: float):\n",
        "    \"\"\"returns a * b.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "def divide(a: float, b: float):\n",
        "    \"\"\"returns a / b.\"\"\"\n",
        "    if b == 0:\n",
        "        return \"Cannot divide by zero.\"\n",
        "    return a / b\n",
        "\n",
        "operation_tools = [add, subtract, multiply, divide]\n",
        "\n",
        "chat = client.chats.create(\n",
        "    model=MODEL_ID,\n",
        "    config={\n",
        "        \"tools\": operation_tools,\n",
        "        \"automatic_function_calling\": {\"disable\": False} # Enabled by default\n",
        "    }\n",
        ")\n",
        "\n",
        "response = chat.send_message(\n",
        "    \"I have 57 cats, each owns 44 mittens, how many mittens is that in total?\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "cU2TO5-S8tmp"
      },
      "outputs": [],
      "source": [
        "print_history(chat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8A6qJ668ywT"
      },
      "source": [
        "Automatic execution handled the `multiply` call seamlessly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BzsV6MxLnZD"
      },
      "source": [
        "## Automatic Function Schema Declaration\n",
        "\n",
        "A key convenience of the Python SDK is its ability to automatically generate the required `FunctionDeclaration` schema from your Python functions. It inspects:\n",
        "\n",
        "- **Function Name**: (`func.__name__`)\n",
        "\n",
        "- **Docstring**: Used for the function's description.\n",
        "\n",
        "- **Parameters**: Names and type annotations (`int`, `str`, `float`, `bool`, `list`, `dict`). Docstrings for parameters (if using specific formats like Google style) can also enhance the description.\n",
        "\n",
        "- **Return Type Annotation**: Although not strictly used by the model for deciding which function to call, it's good practice.\n",
        "\n",
        "You generally don't need to create `FunctionDeclaration` objects manually when using Python functions directly as tools.\n",
        "\n",
        "However, you can generate the schema explicitly using `genai.types.FunctionDeclaration.from_callable` if you need to inspect it, modify it, or use it in scenarios where you don't have the Python function object readily available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "qrYRieAuL2hs"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "set_color_declaration = types.FunctionDeclaration.from_callable(\n",
        "    callable = set_light_color,\n",
        "    client = client\n",
        ")\n",
        "\n",
        "print(json.dumps(set_color_declaration.to_json_dict(), indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eea8e3a0b89f"
      },
      "source": [
        "## Manual function calling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9610f3465a69"
      },
      "source": [
        "For more control, or if automatic function calling is not available,  you can process [`genai.types.FunctionCall`](https://googleapis.github.io/python-genai/genai.html#genai.types.FunctionCall) requests from the model yourself. This would be the case if:\n",
        "\n",
        "- You use a `Chat` with the default `\"automatic_function_calling\": {\"disable\": True}`.\n",
        "- You use [`Client.model.generate_content`](https://googleapis.github.io/python-genai/genai.html#genai.types.) (and manage the chat history yourself)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34ffab0bf365"
      },
      "source": [
        "**Example: Movies**\n",
        "\n",
        "The following example is a rough equivalent of the [function calling single-turn curl sample](https://ai.google.dev/docs/function_calling#function-calling-single-turn-curl-sample) in Python. It uses functions that return (mock) movie playtime information, possibly from a hypothetical API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "46ba0fa3d09a"
      },
      "outputs": [],
      "source": [
        "def find_movies(description: str, location: str):\n",
        "    \"\"\"find movie titles currently playing in theaters based on any description, genre, title words, etc.\n",
        "\n",
        "    Args:\n",
        "        description: Any kind of description including category or genre, title words, attributes, etc.\n",
        "        location: The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\n",
        "    \"\"\"\n",
        "    return [\"Barbie\", \"Oppenheimer\"]\n",
        "\n",
        "\n",
        "def find_theaters(location: str, movie: str):\n",
        "    \"\"\"Find theaters based on location and optionally movie title which are currently playing in theaters.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\n",
        "        movie: Any movie title\n",
        "    \"\"\"\n",
        "    return [\"Googleplex 16\", \"Android Theatre\"]\n",
        "\n",
        "\n",
        "def get_showtimes(location: str, movie: str, theater: str, date: str):\n",
        "    \"\"\"\n",
        "    Find the start times for movies playing in a specific theater.\n",
        "\n",
        "    Args:\n",
        "      location: The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\n",
        "      movie: Any movie title\n",
        "      thearer: Name of the theater\n",
        "      date: Date for requested showtime\n",
        "    \"\"\"\n",
        "    return [\"10:00\", \"11:00\"]\n",
        "\n",
        "theater_functions = [find_movies, find_theaters, get_showtimes]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11631c6e2b10"
      },
      "source": [
        "After using `generate_content()` to ask a question, the model requests a `function_call`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "5e3b9c84d883"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Which theaters in Mountain View, CA show the Barbie movie?\",\n",
        "    config = {\n",
        "        \"tools\": theater_functions,\n",
        "        \"automatic_function_calling\": {\"disable\": True}\n",
        "    }\n",
        ")\n",
        "\n",
        "print(json.dumps(response.candidates[0].content.parts[0].to_json_dict(), indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuldoypuAC1i"
      },
      "source": [
        "Since this is not using a `ChatSession` with automatic function calling, you have to call the function yourself.\n",
        "\n",
        "A very simple way to do this would be with `if` statements:\n",
        "\n",
        "```python\n",
        "if function_call.name == 'find_theaters':\n",
        "  find_theaters(**function_call.args)\n",
        "elif ...\n",
        "```\n",
        "\n",
        "However, since you already made the `theater_functions` list, this can be simplified to:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "rjkZ8MA00Coc"
      },
      "outputs": [],
      "source": [
        "def call_function(function_call, functions):\n",
        "    function_name = function_call.name\n",
        "    function_args = function_call.args\n",
        "    # Find the function object from the list based on the function name\n",
        "    for func in functions:\n",
        "        if func.__name__ == function_name:\n",
        "            return func(**function_args)\n",
        "\n",
        "part = response.candidates[0].content.parts[0]\n",
        "\n",
        "# Check if it's a function call; in real use you'd need to also handle text\n",
        "# responses as you won't know what the model will respond with.\n",
        "if part.function_call:\n",
        "    result = call_function(part.function_call, theater_functions)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLWrHOatBtRz"
      },
      "source": [
        "Finally, pass the response plus the message history to the next `generate_content()` call to get a final text response from the model. The next code cell is showing on purpose different ways to write down `Content` so you can choose the one that you prefer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "xr13VGnJAgZv"
      },
      "outputs": [],
      "source": [
        "from google.genai import types\n",
        "# Build the message history\n",
        "messages = [\n",
        "    types.Content(\n",
        "        role=\"user\",\n",
        "        parts=[\n",
        "            types.Part(\n",
        "                text=\"Which theaters in Mountain View show the Barbie movie?.\"\n",
        "            )\n",
        "        ]\n",
        "    ),\n",
        "    types.Content(\n",
        "        role=\"model\",\n",
        "        parts=[part]\n",
        "    ),\n",
        "    types.Content(\n",
        "        role=\"tool\",\n",
        "        parts=[\n",
        "            types.Part.from_function_response(\n",
        "                name=part.function_call.name,\n",
        "                response={\"output\":result},\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "]\n",
        "\n",
        "# Generate the next response\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=messages,\n",
        "    config = {\n",
        "        \"tools\": theater_functions,\n",
        "        \"automatic_function_calling\": {\"disable\": True}\n",
        "    }\n",
        ")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZFiMVth9Kjb"
      },
      "source": [
        "This demonstrates the manual workflow: call, check, execute, respond, call again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlrmXN7fxQi0"
      },
      "source": [
        "Now call the model with an instruction that could use all of the specified tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxXGT3n4AQhk"
      },
      "source": [
        "## Compositional Function Calling\n",
        "The model can chain function calls across multiple turns, using the result from one call to inform the next. This allows for complex, multi-step reasoning and task completion.\n",
        "\n",
        "**Example: Finding Specific Movie Showtimes**\n",
        "\n",
        "Let's reuse the theater_functions and ask a more complex query that requires finding movies first, then potentially theaters, then showtimes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "1jGiexKsAolU"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(\n",
        "    model = MODEL_ID,\n",
        "    config = {\n",
        "        \"tools\": theater_functions,\n",
        "    }\n",
        ")\n",
        "\n",
        "response = chat.send_message(\"\"\"\n",
        "  Find comedy movies playing in Mountain View, CA on 01/01/2025.\n",
        "  First, find the movie titles.\n",
        "  Then, find the theaters showing those movies.\n",
        "  Finally, find the showtimes for each movie at each theater.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)\n",
        "print(\"\\n--- History ---\")\n",
        "print_history(chat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7J-nyTN8hL-"
      },
      "source": [
        "Here you can see that the model made seven calls to answer your question and used the outputs of them in the subsequent calls and in the final answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsS8_hhNBpLS"
      },
      "source": [
        "## Function Calling Configuration using Modes\n",
        "\n",
        "While AUTO mode (or the SDK's default automatic execution) is often sufficient, you can precisely control when and which functions the model is allowed to call using the tool_config parameter during model/chat initialization or in send_message.\n",
        "\n",
        "The `tool_config` accepts a ToolConfig object, which contains a `FunctionCallingConfig`.\n",
        "\n",
        "The `FunctionCallingConfig` has two main fields:\n",
        "\n",
        "- `mode`: Controls the overall function calling behavior (AUTO, ANY, NONE).\n",
        "\n",
        "- `allowed_function_names`: An optional list of function names the model is restricted to calling in this turn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGZtasE4CObk"
      },
      "source": [
        "### AUTO (Default Mode)\n",
        "\n",
        "- Behavior: The model decides whether to respond with text or to call one or more functions from the provided `tools`. This is the most flexible mode.\n",
        "\n",
        "- SDK Default: When using ChatSession with automatic execution enabled, the underlying behavior effectively uses `AUTO` mode unless overridden by `tool_config`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "mggqLU55CZlj"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(model=MODEL_ID)\n",
        "\n",
        "response = chat.send_message(\n",
        "    message=\"Turn on the lights!\",\n",
        "    config={\n",
        "        \"system_instruction\": instruction,\n",
        "        \"tools\": light_controls,\n",
        "        \"tool_config\" : types.ToolConfig(\n",
        "            function_calling_config=types.FunctionCallingConfig(\n",
        "                mode=\"auto\"\n",
        "            )\n",
        "        )\n",
        "    }\n",
        ")\n",
        "\n",
        "print_history(chat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Yw5Vn3y3DkG6"
      },
      "outputs": [],
      "source": [
        "none_chat = client.chats.create(model=MODEL_ID)\n",
        "\n",
        "response = none_chat.send_message(\n",
        "    message=\"Hello light-bot, what can you do?\",\n",
        "    config={\n",
        "        \"system_instruction\": instruction,\n",
        "        \"tools\": light_controls, # Tools are provided\n",
        "        \"tool_config\" : types.ToolConfig(\n",
        "            function_calling_config=types.FunctionCallingConfig(\n",
        "                mode=\"none\"\n",
        "            )\n",
        "        ) # but NONE mode prevents their use\n",
        "    }\n",
        ")\n",
        "\n",
        "print_history(none_chat)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "hY2NtS3jV56U"
      ],
      "name": "Function_calling.ipynb",
      "toc_visible": true
    },
    "google": {
      "image_path": "/site-assets/images/share.png",
      "keywords": [
        "examples",
        "googleai",
        "samplecode",
        "python",
        "embed",
        "function"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
